{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sudarshan-koirala/langchain-falcon-chainlit/blob/main/langchain_falcon.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langchain huggingface_hub watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"Sudarshan Koirala\" -vmp langchain,huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your Huggingface access token from https://huggingface.co/settings/tokens ðŸ”‘\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "HUGGINGFACE_API_TOKEN = \"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\"#getpass()\n",
    "os.environ[\"HUGGINGFACE_API_TOKEN\"] = HUGGINGFACE_API_TOKEN   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use falcon-7b-instruct model from [Huggingface website](https://huggingface.co/tiiuae/falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/py310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN, \n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI assistant, I would recommend checking out the Louvre Abu Dhabi for some amazing art and architecture. The Abu Dhabi Falcon Festival is also a great experience to attend. For some relaxation, try the Abu Dhabi beach or the pool at your hotel. Enjoy your holidays!\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant and provide the answer for the question asked politely.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/py310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN, \n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700})\n",
    "\n",
    "# Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "#prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(llm=llm,prompt=prompt, verbose = True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
      "\n",
      "Previous conversation:\n",
      "\n",
      "\n",
      "New human question: Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\n",
      "Response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Of course! Abu Dhabi has a lot to offer. For a cultural experience, I would recommend visiting the Sheikh Zayed Mosque or the Al Zafran Heritage Village. For fun and entertainment, try visiting the Mall of Abu Dhabi or the Aquaventure Waterpark. And for relaxation, the beach at Abu Dhabi is a great place to spend your days.\n",
      "\n",
      "New human: \n"
     ]
    }
   ],
   "source": [
    "question = \"Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\"\n",
    "\n",
    "print(llm_chain.predict(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
      "\n",
      "Previous conversation:\n",
      "Human: Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\n",
      "AI:  Of course! Abu Dhabi has a lot to offer. For a cultural experience, I would recommend visiting the Sheikh Zayed Mosque or the Al Zafran Heritage Village. For fun and entertainment, try visiting the Mall of Abu Dhabi or the Aquaventure Waterpark. And for relaxation, the beach at Abu Dhabi is a great place to spend your days.\n",
      "\n",
      "New human: \n",
      "\n",
      "New human question: and what is the best season to visiting it?\n",
      "Response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Visiting Abu Dhabi during the winter months, from November to February, is the best time to avoid the extreme heat and try a number of popular attractions, such as the Desert Safari, the Abu Dhabi Grand Prix, and the Abu Dhabi International Food Festival.\n"
     ]
    }
   ],
   "source": [
    "question = \"and what is the best season to visiting it?\"\n",
    "\n",
    "print(llm_chain.predict(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
